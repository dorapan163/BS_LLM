{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 630,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.047619047619047616,
      "grad_norm": 1.6408240795135498,
      "learning_rate": 1.4062500000000001e-05,
      "loss": 4.285,
      "step": 10
    },
    {
      "epoch": 0.09523809523809523,
      "grad_norm": 1.3958549499511719,
      "learning_rate": 2.96875e-05,
      "loss": 4.2452,
      "step": 20
    },
    {
      "epoch": 0.14285714285714285,
      "grad_norm": 1.5277302265167236,
      "learning_rate": 4.5312500000000004e-05,
      "loss": 3.9733,
      "step": 30
    },
    {
      "epoch": 0.19047619047619047,
      "grad_norm": 1.3753031492233276,
      "learning_rate": 4.99830973606048e-05,
      "loss": 3.6352,
      "step": 40
    },
    {
      "epoch": 0.23809523809523808,
      "grad_norm": 1.4105050563812256,
      "learning_rate": 4.9900363939833714e-05,
      "loss": 3.3819,
      "step": 50
    },
    {
      "epoch": 0.2857142857142857,
      "grad_norm": 1.8418017625808716,
      "learning_rate": 4.974892317121368e-05,
      "loss": 3.1375,
      "step": 60
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 2.2463173866271973,
      "learning_rate": 4.952919292388078e-05,
      "loss": 2.9987,
      "step": 70
    },
    {
      "epoch": 0.38095238095238093,
      "grad_norm": 2.22977352142334,
      "learning_rate": 4.9241779497508924e-05,
      "loss": 2.9883,
      "step": 80
    },
    {
      "epoch": 0.42857142857142855,
      "grad_norm": 3.3278872966766357,
      "learning_rate": 4.888747594935258e-05,
      "loss": 2.8386,
      "step": 90
    },
    {
      "epoch": 0.47619047619047616,
      "grad_norm": 2.106642961502075,
      "learning_rate": 4.846725990597122e-05,
      "loss": 2.7299,
      "step": 100
    },
    {
      "epoch": 0.5238095238095238,
      "grad_norm": 1.9903807640075684,
      "learning_rate": 4.7982290865673116e-05,
      "loss": 2.7391,
      "step": 110
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 2.257030487060547,
      "learning_rate": 4.743390699912232e-05,
      "loss": 2.8698,
      "step": 120
    },
    {
      "epoch": 0.6190476190476191,
      "grad_norm": 2.3214120864868164,
      "learning_rate": 4.6823621456936397e-05,
      "loss": 2.7855,
      "step": 130
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 1.9429709911346436,
      "learning_rate": 4.615311819446378e-05,
      "loss": 2.6287,
      "step": 140
    },
    {
      "epoch": 0.7142857142857143,
      "grad_norm": 2.0791666507720947,
      "learning_rate": 4.542424732526105e-05,
      "loss": 2.6704,
      "step": 150
    },
    {
      "epoch": 0.7619047619047619,
      "grad_norm": 1.9443548917770386,
      "learning_rate": 4.4639020016091395e-05,
      "loss": 2.6773,
      "step": 160
    },
    {
      "epoch": 0.8095238095238095,
      "grad_norm": 2.1727781295776367,
      "learning_rate": 4.379960293753046e-05,
      "loss": 2.499,
      "step": 170
    },
    {
      "epoch": 0.8571428571428571,
      "grad_norm": 2.2569520473480225,
      "learning_rate": 4.290831228549196e-05,
      "loss": 2.6175,
      "step": 180
    },
    {
      "epoch": 0.9047619047619048,
      "grad_norm": 2.076007604598999,
      "learning_rate": 4.1967607390169295e-05,
      "loss": 2.5154,
      "step": 190
    },
    {
      "epoch": 0.9523809523809523,
      "grad_norm": 2.199414014816284,
      "learning_rate": 4.0980083930028154e-05,
      "loss": 2.3834,
      "step": 200
    },
    {
      "epoch": 1.0,
      "grad_norm": 2.441312313079834,
      "learning_rate": 3.994846676957448e-05,
      "loss": 2.6089,
      "step": 210
    },
    {
      "epoch": 1.0476190476190477,
      "grad_norm": 2.2202987670898438,
      "learning_rate": 3.8875602440660634e-05,
      "loss": 2.4841,
      "step": 220
    },
    {
      "epoch": 1.0952380952380953,
      "grad_norm": 2.6952078342437744,
      "learning_rate": 3.776445128807594e-05,
      "loss": 2.4782,
      "step": 230
    },
    {
      "epoch": 1.1428571428571428,
      "grad_norm": 1.999028205871582,
      "learning_rate": 3.6618079301094216e-05,
      "loss": 2.6379,
      "step": 240
    },
    {
      "epoch": 1.1904761904761905,
      "grad_norm": 2.5060319900512695,
      "learning_rate": 3.5439649653517414e-05,
      "loss": 2.5147,
      "step": 250
    },
    {
      "epoch": 1.2380952380952381,
      "grad_norm": 2.3688149452209473,
      "learning_rate": 3.423241397555893e-05,
      "loss": 2.458,
      "step": 260
    },
    {
      "epoch": 1.2857142857142856,
      "grad_norm": 2.3673176765441895,
      "learning_rate": 3.299970338164995e-05,
      "loss": 2.4489,
      "step": 270
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 2.7428343296051025,
      "learning_rate": 3.1744919278925605e-05,
      "loss": 2.4764,
      "step": 280
    },
    {
      "epoch": 1.380952380952381,
      "grad_norm": 2.3957037925720215,
      "learning_rate": 3.0471523981753264e-05,
      "loss": 2.4339,
      "step": 290
    },
    {
      "epoch": 1.4285714285714286,
      "grad_norm": 1.9477306604385376,
      "learning_rate": 2.918303115819992e-05,
      "loss": 2.3769,
      "step": 300
    },
    {
      "epoch": 1.4761904761904763,
      "grad_norm": 2.4660589694976807,
      "learning_rate": 2.7882996134799855e-05,
      "loss": 2.4157,
      "step": 310
    },
    {
      "epoch": 1.5238095238095237,
      "grad_norm": 2.5470519065856934,
      "learning_rate": 2.6575006086374476e-05,
      "loss": 2.3636,
      "step": 320
    },
    {
      "epoch": 1.5714285714285714,
      "grad_norm": 2.3846490383148193,
      "learning_rate": 2.526267013797341e-05,
      "loss": 2.4442,
      "step": 330
    },
    {
      "epoch": 1.619047619047619,
      "grad_norm": 2.4206111431121826,
      "learning_rate": 2.394960940624858e-05,
      "loss": 2.3768,
      "step": 340
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 1.9453338384628296,
      "learning_rate": 2.2639447007739934e-05,
      "loss": 2.5019,
      "step": 350
    },
    {
      "epoch": 1.7142857142857144,
      "grad_norm": 2.4061732292175293,
      "learning_rate": 2.1335798061642956e-05,
      "loss": 2.4522,
      "step": 360
    },
    {
      "epoch": 1.7619047619047619,
      "grad_norm": 2.4929606914520264,
      "learning_rate": 2.004225971464308e-05,
      "loss": 2.4488,
      "step": 370
    },
    {
      "epoch": 1.8095238095238095,
      "grad_norm": 2.572244167327881,
      "learning_rate": 1.8762401215341567e-05,
      "loss": 2.4503,
      "step": 380
    },
    {
      "epoch": 1.8571428571428572,
      "grad_norm": 2.759045362472534,
      "learning_rate": 1.749975406566029e-05,
      "loss": 2.4046,
      "step": 390
    },
    {
      "epoch": 1.9047619047619047,
      "grad_norm": 2.5103957653045654,
      "learning_rate": 1.62578022764006e-05,
      "loss": 2.3952,
      "step": 400
    },
    {
      "epoch": 1.9523809523809523,
      "grad_norm": 2.270409345626831,
      "learning_rate": 1.5039972753843965e-05,
      "loss": 2.5508,
      "step": 410
    },
    {
      "epoch": 2.0,
      "grad_norm": 2.45955491065979,
      "learning_rate": 1.3849625843920634e-05,
      "loss": 2.4533,
      "step": 420
    },
    {
      "epoch": 2.0476190476190474,
      "grad_norm": 2.654921770095825,
      "learning_rate": 1.2690046060037661e-05,
      "loss": 2.4141,
      "step": 430
    },
    {
      "epoch": 2.0952380952380953,
      "grad_norm": 2.6321706771850586,
      "learning_rate": 1.1564433020150944e-05,
      "loss": 2.3633,
      "step": 440
    },
    {
      "epoch": 2.142857142857143,
      "grad_norm": 2.1945903301239014,
      "learning_rate": 1.0475892618088426e-05,
      "loss": 2.4783,
      "step": 450
    },
    {
      "epoch": 2.1904761904761907,
      "grad_norm": 2.544621229171753,
      "learning_rate": 9.427428453485574e-06,
      "loss": 2.4583,
      "step": 460
    },
    {
      "epoch": 2.238095238095238,
      "grad_norm": 2.6596906185150146,
      "learning_rate": 8.421933543980126e-06,
      "loss": 2.4168,
      "step": 470
    },
    {
      "epoch": 2.2857142857142856,
      "grad_norm": 2.3347816467285156,
      "learning_rate": 7.462182342534895e-06,
      "loss": 2.3799,
      "step": 480
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 3.113662004470825,
      "learning_rate": 6.5508230819148915e-06,
      "loss": 2.4288,
      "step": 490
    },
    {
      "epoch": 2.380952380952381,
      "grad_norm": 2.765894651412964,
      "learning_rate": 5.690370467442744e-06,
      "loss": 2.3423,
      "step": 500
    },
    {
      "epoch": 2.4285714285714284,
      "grad_norm": 2.643155813217163,
      "learning_rate": 4.883198738195158e-06,
      "loss": 2.4577,
      "step": 510
    },
    {
      "epoch": 2.4761904761904763,
      "grad_norm": 2.5120174884796143,
      "learning_rate": 4.1315351157866e-06,
      "loss": 2.3936,
      "step": 520
    },
    {
      "epoch": 2.5238095238095237,
      "grad_norm": 2.51407790184021,
      "learning_rate": 3.437453658816994e-06,
      "loss": 2.4216,
      "step": 530
    },
    {
      "epoch": 2.571428571428571,
      "grad_norm": 2.6932413578033447,
      "learning_rate": 2.8028695399406195e-06,
      "loss": 2.3297,
      "step": 540
    },
    {
      "epoch": 2.619047619047619,
      "grad_norm": 2.36956524848938,
      "learning_rate": 2.2295337613476714e-06,
      "loss": 2.4155,
      "step": 550
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 2.9661526679992676,
      "learning_rate": 1.719028323239802e-06,
      "loss": 2.4083,
      "step": 560
    },
    {
      "epoch": 2.7142857142857144,
      "grad_norm": 2.180018901824951,
      "learning_rate": 1.2727618586313494e-06,
      "loss": 2.4129,
      "step": 570
    },
    {
      "epoch": 2.761904761904762,
      "grad_norm": 2.2735707759857178,
      "learning_rate": 8.919657465210868e-07,
      "loss": 2.3243,
      "step": 580
    },
    {
      "epoch": 2.8095238095238093,
      "grad_norm": 2.7741005420684814,
      "learning_rate": 5.776907141593235e-07,
      "loss": 2.3263,
      "step": 590
    },
    {
      "epoch": 2.857142857142857,
      "grad_norm": 2.3607053756713867,
      "learning_rate": 3.30803937785773e-07,
      "loss": 2.468,
      "step": 600
    },
    {
      "epoch": 2.9047619047619047,
      "grad_norm": 2.3804616928100586,
      "learning_rate": 1.5198664983802347e-07,
      "loss": 2.4338,
      "step": 610
    },
    {
      "epoch": 2.9523809523809526,
      "grad_norm": 2.607823371887207,
      "learning_rate": 4.1732259233071e-08,
      "loss": 2.3848,
      "step": 620
    },
    {
      "epoch": 3.0,
      "grad_norm": 3.0967800617218018,
      "learning_rate": 3.4498990858777835e-10,
      "loss": 2.3817,
      "step": 630
    }
  ],
  "logging_steps": 10,
  "max_steps": 630,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.3434057516843008e+17,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
